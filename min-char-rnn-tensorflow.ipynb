{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data = open('input.txt', 'r').read()  # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "char2idx = { ch:i for i, ch in enumerate(chars)}\n",
    "idx2char = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([63,  0, 62, ...,  0, 25, 51]), array([ 0, 62, 24, ..., 25, 51, 44]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_raw_data():\n",
    "    data_as_int = np.array(list(map(char2idx.get,data)))\n",
    "    return data_as_int[0:-1],data_as_int[1:]\n",
    "\n",
    "get_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "batch_size = 5\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "def get_batch_seq(data):\n",
    "    raw_x, raw_y = data\n",
    "    batch_partition_length = len(raw_x) // batch_size\n",
    "    print(batch_partition_length)\n",
    "    print(raw_x[:-(len(raw_x)%batch_partition_length)])\n",
    "    data_x=raw_x[:-(len(raw_x)%batch_partition_length)].reshape(-1,batch_partition_length)\n",
    "    data_y=raw_y[:-(len(raw_x)%batch_partition_length)].reshape(-1,batch_partition_length)\n",
    "    \n",
    "    epoch_steps = batch_partition_length // seq_length\n",
    "    for step in range(epoch_steps):        \n",
    "        x = data_x[:, step*seq_length:(step+1)*seq_length]\n",
    "        y = data_y[:, step*seq_length:(step+1)*seq_length]\n",
    "        yield x,y           \n",
    "\n",
    "def get_epoch(n):\n",
    "    for i in range(n):\n",
    "        yield get_batch_seq(get_raw_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223078\n",
      "[63  0 62 ..., 34 49  0]\n",
      "Average loss at step 1000 for last 250 steps: 26.3667043316\n",
      "Average loss at step 2000 for last 250 steps: 22.5846868491\n",
      "Average loss at step 3000 for last 250 steps: 21.4778093135\n",
      "Average loss at step 4000 for last 250 steps: 21.0897589529\n",
      "Average loss at step 5000 for last 250 steps: 20.583658998\n",
      "Average loss at step 6000 for last 250 steps: 20.0866882157\n",
      "Average loss at step 7000 for last 250 steps: 19.6524748802\n",
      "Average loss at step 8000 for last 250 steps: 19.2783337593\n",
      "223078\n",
      "[63  0 62 ..., 34 49  0]\n",
      "Average loss at step 1000 for last 250 steps: 19.0165853751\n",
      "Average loss at step 2000 for last 250 steps: 18.8002626002\n",
      "Average loss at step 3000 for last 250 steps: 18.4381797528\n",
      "Average loss at step 4000 for last 250 steps: 18.7796692431\n",
      "Average loss at step 5000 for last 250 steps: 18.6476110995\n",
      "Average loss at step 6000 for last 250 steps: 18.3300234425\n",
      "Average loss at step 7000 for last 250 steps: 18.1147738588\n",
      "Average loss at step 8000 for last 250 steps: 17.9182780552\n",
      "train finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'F Out the stand the stand the stand the stand the stand the stand the stand the stand the stand the '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CharRnnModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def create_compute_graph(self):\n",
    "\n",
    "        with tf.variable_scope(str(id(self)) + 'rnn_cell'):\n",
    "            w = tf.get_variable('w',[vocab_size + state_size, state_size])\n",
    "            b = tf.get_variable('b',[state_size],initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        def rnn_cell(rnn_input,pre_state):\n",
    "\n",
    "            with tf.variable_scope(str(id(self)) + 'rnn_cell',reuse=True):\n",
    "                w = tf.get_variable('w',[vocab_size + state_size, state_size])\n",
    "                b = tf.get_variable('b',[state_size],initializer=tf.constant_initializer(0.0))\n",
    "            return tf.tanh(tf.matmul(tf.concat([rnn_input,pre_state],axis=1), w) + b)\n",
    "        \n",
    "        # def create_compute_graph():\n",
    "        x = tf.placeholder(tf.int32, [None, seq_length])\n",
    "        y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "        init_state = tf.placeholder(tf.float32,[None, state_size])\n",
    "\n",
    "        x_one_hot = tf.one_hot(x,vocab_size)\n",
    "        y_one_hot = tf.one_hot(y,vocab_size)\n",
    "\n",
    "        rnn_inputs = tf.unstack(x_one_hot,axis=1)\n",
    "        rnn_labels = tf.unstack(y_one_hot,axis=1)\n",
    "\n",
    "        state = init_state\n",
    "        rnn_outputs = []\n",
    "        for rnn_input in rnn_inputs:\n",
    "            state = rnn_cell(rnn_input, state)\n",
    "            rnn_outputs.append(state)\n",
    "        final_state = state\n",
    "\n",
    "        with tf.variable_scope(str(id(self)) + 'softmax'):\n",
    "            w = tf.get_variable('w',[state_size, vocab_size])\n",
    "            b = tf.get_variable('b',[vocab_size],initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        logits = [tf.matmul(rnn_output, w) + b for rnn_output in rnn_outputs]\n",
    "        #predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "        losses = [tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logit) \\\n",
    "                  for logit, label in zip(logits, rnn_labels)]\n",
    "        total_loss = tf.reduce_mean(losses)\n",
    "        update = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "        \n",
    "        return x,y,init_state,final_state,total_loss,update\n",
    "    \n",
    "    def train(self,num_epochs):\n",
    "        x,y,init_state,final_state,total_loss,update = self.create_compute_graph()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        training_losses=[]\n",
    "        for epoch in get_epoch(num_epochs):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                training_loss_, training_state, _ = self.sess.run([total_loss,final_state,update],\n",
    "                                                             feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 1000 == 0 and step > 0:\n",
    "                    print(\"Average loss at step\", step,\n",
    "                          \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "        print('train finished')\n",
    "        return training_losses\n",
    "    \n",
    "    def create_test_graph(self):\n",
    "        x = tf.placeholder(tf.int32,[1])\n",
    "        x_one_hot = tf.one_hot(x,vocab_size)\n",
    "        init_state = tf.placeholder(tf.float32,[1,state_size])\n",
    "        \n",
    "        with tf.variable_scope(str(id(self)) + 'rnn_cell',reuse=True):\n",
    "            w = tf.get_variable('w',[vocab_size + state_size, state_size])\n",
    "            b = tf.get_variable('b',[state_size],initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "        state = tf.tanh(tf.matmul(tf.concat([x_one_hot,init_state],axis=1),w) + b)\n",
    "        \n",
    "        with tf.variable_scope(str(id(self)) + 'softmax', reuse=True):\n",
    "            w2 = tf.get_variable('w',[state_size, vocab_size])\n",
    "            b2 = tf.get_variable('b',[vocab_size],initializer=tf.constant_initializer(0.0))\n",
    "        y = tf.matmul(state,w2) + b2\n",
    "        p = tf.nn.softmax(y)\n",
    "        out = tf.argmax(p,axis=1) \n",
    "        return x, init_state,state, out\n",
    "    \n",
    "    def sample(self,n):\n",
    "        x, init_state, state,out = self.create_test_graph()\n",
    "        test_x = np.array([char2idx.get(data[0])])\n",
    "        training_state = np.zeros([1,state_size])\n",
    "        result = []\n",
    "        for i in range(n):\n",
    "            result.append(test_x[0])\n",
    "            training_state,test_x = self.sess.run([state,out],feed_dict = {x:test_x, \n",
    "                                                                           init_state:training_state})\n",
    "        return \"\".join(list(map(lambda x:idx2char[x],result)))\n",
    "        \n",
    "    \n",
    "model = CharRnnModel()    \n",
    "model.train(1)\n",
    "\n",
    "model.sample(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
